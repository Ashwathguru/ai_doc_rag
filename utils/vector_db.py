import faiss
import numpy as np
import ollama
from utils.embedding import encode_question

class VectorDB:
    def __init__(self):
        # Initialize FAISS index
        self.index = faiss.IndexFlatL2(384)  # Dimension of embeddings (e.g., 384 for MiniLM)
        self.conversations = []

    def store_embeddings(self, embeddings):
        # Convert embeddings to numpy array and add to FAISS index
        embeddings = np.array(embeddings).astype('float32')
        self.index.add(embeddings)

    def query(self, question):
        # Generate the question embedding using the SentenceTransformer model
        question_embedding = encode_question(question)

        # Perform the search on FAISS index for the most relevant context
        D, I = self.index.search(question_embedding, k=5)  # Search for top 5 relevant embeddings
        
        # Construct a context for Llama 2 from the most relevant document chunks
        context = self._get_context_from_index(I)
        
        # Get response from Llama 2 (via Ollama)
        response = self._query_ollama(question, context)
        return response

    def store_conversation(self, question, answer):
        # Store questions and answers for context awareness
        self.conversations.append({"question": question, "answer": answer})
        
    def _get_context_from_index(self, indices):
        # Generate context for the query by getting relevant document chunks
        context = ""
        for idx in indices[0]:
            # Add relevant document content (here, simulate with placeholders)
            context += f"Relevant document chunk {idx} text.\n"  # Placeholder: Replace with actual text from your DB
        return context

    def _query_ollama(self, question, context):
        # Query the Llama 2 model using Ollama
        prompt = f"Context: {context}\n\nQuestion: {question}\nAnswer:"
        
        # Make a call to Ollama API (Llama 2 model)
        response = ollama.chat(model="llama2", messages=[{"role": "user", "content": prompt}])
        
        # Return the response generated by Llama 2
        return response["message"]["content"]
