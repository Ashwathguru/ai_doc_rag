import faiss
import numpy as np
import ollama
from utils.embedding import encode_question

class VectorDB:
    def __init__(self):
        # Initialize FAISS index
        self.index = faiss.IndexFlatL2(384)  # Dimension of embeddings (e.g., 384 for MiniLM)
        self.documents = []  # Store document chunks alongside embeddings for context retrieval
        self.conversations = []

    def store_embeddings(self, embeddings, docs):
        """
        Store embeddings in the FAISS index and map them to the corresponding document chunks.
        Args:
            embeddings: List of embedding vectors.
            docs: List of document chunks corresponding to the embeddings.
        """
        embeddings = np.array(embeddings).astype('float32')
        self.index.add(embeddings)  # Add embeddings to FAISS
        self.documents.extend(docs)  # Maintain a parallel list of document chunks
        print("Stored embeddings in the FAISS index !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    
    def query(self, question):
        """
        Query the FAISS index with a question to find the most relevant document chunks.
        Args:
            question: The user query string.
        Returns:
            Response from the Llama 2 model (via Ollama) or an error message if the index is empty.
        """
        # Check if the index contains any embeddings
        if self.index.ntotal == 0:
            return "The database is empty. Please upload a document and try again."

        # Generate the question embedding
        question_embedding = encode_question(question)
        
        # Ensure the embedding is a 2D array
        question_embedding = np.array(question_embedding).reshape(1, -1).astype('float32')

        # Perform FAISS search for the top 5 relevant embeddings
        D, I = self.index.search(question_embedding, k=5)

        # Retrieve the most relevant document chunks
        context = self._get_context_from_index(I)

        # Query Llama 2 model (via Ollama) with the context
        response = self._query_ollama(question, context)
        return response

    def store_conversation(self, question, answer):
        """
        Store the conversation history for context awareness.
        Args:
            question: User question.
            answer: Model-generated answer.
        """
        self.conversations.append({"question": question, "answer": answer})

    def _get_context_from_index(self, indices):
        """
        Retrieve document chunks corresponding to FAISS index results.
        Args:
            indices: Indices of the top retrieved embeddings from FAISS.
        Returns:
            Combined document chunks as a context string.
        """
        context = ""
        for idx in indices[0]:
            if 0 <= idx < len(self.documents):
                context += self.documents[idx] + "\n"
        return context.strip()

    def _query_ollama(self, question, context):
        """
        Query the Llama 2 model via Ollama.
        Args:
            question: User question.
            context: Context string constructed from retrieved document chunks.
        Returns:
            The response generated by the Llama 2 model.
        """
        prompt = f"Context: {context}\n\nQuestion: {question}\nAnswer:"
        
        # Make a call to the Ollama API (Llama 2 model)
        response = ollama.chat(model="llama2", messages=[{"role": "user", "content": prompt}])
        
        return response["message"]["content"]
